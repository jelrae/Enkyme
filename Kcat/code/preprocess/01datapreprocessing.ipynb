{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfs/omics/personal/jearle/miniconda3/envs/ee/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory /home/jearle/.config/bioservices \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 13:43:18.331694: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-29 13:43:22.997705: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-29 13:43:33.524998: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-29 13:43:33.525412: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-29 13:43:33.525423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from ete3 import NCBITaxa\n",
    "import random\n",
    "random.seed(10)\n",
    "import torch\n",
    "import esm\n",
    "from bioservices import *\n",
    "from functions_and_dicts_data_preprocessing_GNN import *\n",
    "from build_GNN import *\n",
    "import warnings\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "warnings.filterwarnings('ignore')\n",
    "datasets_dir = \"../../data\"\n",
    "\n",
    "CURRENT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading in Sabio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Sabio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 1344\n",
      "Number of UniProt IDs: 370\n"
     ]
    }
   ],
   "source": [
    "organism = \"Seed plants\"\n",
    "\n",
    "df_Sabio = pd.read_table(join(datasets_dir, \"kcat_model_\" + organism + \".tsv\"))\n",
    "\n",
    "df_Sabio[\"kcat\"] = df_Sabio[\"kcat\"].astype('float')\n",
    "df_Sabio[\"PMID\"] = df_Sabio[\"PMID\"].astype('Int64')\n",
    "\n",
    "df_Sabio[\"substrate_IDs\"] = df_Sabio[\"substrate_IDs\"].str.split('#').apply(set)\n",
    "df_Sabio[\"product_IDs\"] = df_Sabio[\"product_IDs\"].str.split('#').apply(set)\n",
    "\n",
    "df_Sabio[\"Type\"][df_Sabio['Type'].str.contains(\"wildtype\")] = \"wildtype\"\n",
    "df_Sabio[\"Type\"][df_Sabio['Type'].str.contains(\"mutant\")] = \"mutant\"\n",
    "\n",
    "print(\"Number of data points: %s\" % len(df_Sabio))\n",
    "print(\"Number of UniProt IDs: %s\" % len(set(df_Sabio[\"Uniprot IDs\"])))\n",
    "\n",
    "df_kcat = df_Sabio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist = []\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    UID, kcat = df_kcat[\"Uniprot IDs\"][ind], df_kcat[\"kcat\"][ind]\n",
    "    help_df = df_kcat.loc[df_kcat[\"Uniprot IDs\"] == UID].loc[df_kcat[\"kcat\"] == kcat]\n",
    "    \n",
    "    if len(help_df) > 1:\n",
    "        droplist = droplist + list(help_df.index)[1:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 104 data points, because they are duplicated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECs</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Uniprot IDs</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Type</th>\n",
       "      <th>kcat</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>pH</th>\n",
       "      <th>Substrates</th>\n",
       "      <th>Products</th>\n",
       "      <th>substrate_IDs</th>\n",
       "      <th>product_IDs</th>\n",
       "      <th>Main Substrate</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Petunia hybrida</td>\n",
       "      <td>Q15GI3</td>\n",
       "      <td>16782809</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>Coniferyl acetate;NADPH</td>\n",
       "      <td>Acetate;NADP+;Isoeugenol</td>\n",
       "      <td>{InChI=1S/C21H30N7O17P3/c22-17-12-19(25-7-24-1...</td>\n",
       "      <td>{InChI=1S/C10H12O2/c1-3-4-8-5-6-9(11)10(7-8)12...</td>\n",
       "      <td>InChI=1S/C12H14O4/c1-9(13)16-7-3-4-10-5-6-11(1...</td>\n",
       "      <td>MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ocimum basilicum</td>\n",
       "      <td>Q15GI4</td>\n",
       "      <td>16782809</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NADPH;Coniferyl acetate</td>\n",
       "      <td>Eugenol;NADP+;Acetate</td>\n",
       "      <td>{InChI=1S/C21H30N7O17P3/c22-17-12-19(25-7-24-1...</td>\n",
       "      <td>{InChI=1S/C10H12O2/c1-3-4-8-5-6-9(11)10(7-8)12...</td>\n",
       "      <td>InChI=1S/C12H14O4/c1-9(13)16-7-3-4-10-5-6-11(1...</td>\n",
       "      <td>MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1.1</td>\n",
       "      <td>Cochlearia officinalis</td>\n",
       "      <td>A7DY56</td>\n",
       "      <td>24583623</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NADPH;3-Methylcyclohexanone;H+</td>\n",
       "      <td>NADP+;3-Methylcyclohexanol</td>\n",
       "      <td>{InChI=1S/p+1, InChI=1S/C7H12O/c1-6-3-2-4-7(8)...</td>\n",
       "      <td>{InChI=1S/C7H14O/c1-6-3-2-4-7(8)5-6/h6-8H,2-5H...</td>\n",
       "      <td>InChI=1S/C21H30N7O17P3/c22-17-12-19(25-7-24-17...</td>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.1.1</td>\n",
       "      <td>Cochlearia officinalis</td>\n",
       "      <td>A7DY56</td>\n",
       "      <td>24583623</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3-Methylcyclohexanone;H+;NADH</td>\n",
       "      <td>NAD+;3-Methylcyclohexanol</td>\n",
       "      <td>{InChI=1S/p+1, InChI=1S/C7H12O/c1-6-3-2-4-7(8)...</td>\n",
       "      <td>{InChI=1S/C7H14O/c1-6-3-2-4-7(8)5-6/h6-8H,2-5H...</td>\n",
       "      <td>InChI=1S/C21H29N7O14P2/c22-17-12-19(25-7-24-17...</td>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.1.1</td>\n",
       "      <td>Cochlearia officinalis</td>\n",
       "      <td>A7DY56</td>\n",
       "      <td>24583623</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>3-Methylcyclohexanol;NADP+</td>\n",
       "      <td>3-Methylcyclohexanone;H+;NADPH</td>\n",
       "      <td>{InChI=1S/C7H14O/c1-6-3-2-4-7(8)5-6/h6-8H,2-5H...</td>\n",
       "      <td>{InChI=1S/p+1, InChI=1S/C7H12O/c1-6-3-2-4-7(8)...</td>\n",
       "      <td>InChI=1S/C21H28N7O17P3/c22-17-12-19(25-7-24-17...</td>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>6.3.2.2</td>\n",
       "      <td>Arabidopsis thaliana</td>\n",
       "      <td>P46309</td>\n",
       "      <td>15180996</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>0.101667</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>L-Glutamate;L-Cysteine;ATP</td>\n",
       "      <td>Phosphate;ADP;gamma-L-Glutamyl-L-cysteine</td>\n",
       "      <td>{InChI=1S/C10H16N5O13P3/c11-8-5-9(13-2-12-8)15...</td>\n",
       "      <td>{InChI=1S/H3O4P/c1-5(2,3)4/h(H3,1,2,3,4), InCh...</td>\n",
       "      <td>InChI=1S/C5H9NO4/c6-3(5(9)10)1-2-4(7)8/h3H,1-2...</td>\n",
       "      <td>MALLSQAGGSYTVVPSGVCSKAGTKAVVSGGVRNLDVLRMKEAFGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>6.3.2.2</td>\n",
       "      <td>Arabidopsis thaliana</td>\n",
       "      <td>P46309</td>\n",
       "      <td>15180996</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>ATP;L-Glutamate;L-Cysteine</td>\n",
       "      <td>gamma-L-Glutamyl-L-cysteine;ADP;Phosphate</td>\n",
       "      <td>{InChI=1S/C10H16N5O13P3/c11-8-5-9(13-2-12-8)15...</td>\n",
       "      <td>{InChI=1S/H3O4P/c1-5(2,3)4/h(H3,1,2,3,4), InCh...</td>\n",
       "      <td>InChI=1S/C10H16N5O13P3/c11-8-5-9(13-2-12-8)15(...</td>\n",
       "      <td>MALLSQAGGSYTVVPSGVCSKAGTKAVVSGGVRNLDVLRMKEAFGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>6.3.2.52</td>\n",
       "      <td>Arabidopsis thaliana</td>\n",
       "      <td>Q8GZ29</td>\n",
       "      <td>29462792</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>(-)-Jasmonic acid;Glutamine;ATP</td>\n",
       "      <td>Diphosphate;Jasmonoyl-glutamine;AMP</td>\n",
       "      <td>{InChI=1S/C5H10N2O3/c6-3(5(9)10)1-2-4(7)8/h3H,...</td>\n",
       "      <td>{InChI=1S/H4O7P2/c1-8(2,3)7-9(4,5)6/h(H2,1,2,3...</td>\n",
       "      <td>InChI=1S/C12H18O3/c1-2-3-4-5-10-9(8-12(14)15)6...</td>\n",
       "      <td>MLPKFDPTNQKACLSLLEDLTTNVKQIQDSVLEAILSRNAQTEYLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>6.3.2.52</td>\n",
       "      <td>Arabidopsis thaliana</td>\n",
       "      <td>Q8GZ29</td>\n",
       "      <td>29462792</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Glutamine;ATP;(-)-Jasmonic acid</td>\n",
       "      <td>Diphosphate;AMP;Jasmonoyl-glutamine</td>\n",
       "      <td>{InChI=1S/C5H10N2O3/c6-3(5(9)10)1-2-4(7)8/h3H,...</td>\n",
       "      <td>{InChI=1S/H4O7P2/c1-8(2,3)7-9(4,5)6/h(H2,1,2,3...</td>\n",
       "      <td>InChI=1S/C5H10N2O3/c6-3(5(9)10)1-2-4(7)8/h3H,1...</td>\n",
       "      <td>MLPKFDPTNQKACLSLLEDLTTNVKQIQDSVLEAILSRNAQTEYLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>6.3.4.4</td>\n",
       "      <td>Zea mays</td>\n",
       "      <td>O24578</td>\n",
       "      <td>9193088</td>\n",
       "      <td>wildtype</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>IMP;L-Aspartate;GTP</td>\n",
       "      <td>GDP;N6-(1,2-Dicarboxyethyl)-AMP;Phosphate</td>\n",
       "      <td>{InChI=1S/C10H16N5O14P3/c11-10-13-7-4(8(18)14-...</td>\n",
       "      <td>{InChI=1S/H3O4P/c1-5(2,3)4/h(H3,1,2,3,4), InCh...</td>\n",
       "      <td>InChI=1S/C10H13N4O8P/c15-6-4(1-21-23(18,19)20)...</td>\n",
       "      <td>MSLSTLSHPAAAAAGSGKSLFPAGPAAQSVHFPKARLPVPAAVSAA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ECs                Organism Uniprot IDs      PMID      Type  \\\n",
       "0            1         Petunia hybrida      Q15GI3  16782809  wildtype   \n",
       "1            1        Ocimum basilicum      Q15GI4  16782809  wildtype   \n",
       "2        1.1.1  Cochlearia officinalis      A7DY56  24583623  wildtype   \n",
       "3        1.1.1  Cochlearia officinalis      A7DY56  24583623  wildtype   \n",
       "4        1.1.1  Cochlearia officinalis      A7DY56  24583623  wildtype   \n",
       "...        ...                     ...         ...       ...       ...   \n",
       "1235   6.3.2.2    Arabidopsis thaliana      P46309  15180996  wildtype   \n",
       "1236   6.3.2.2    Arabidopsis thaliana      P46309  15180996  wildtype   \n",
       "1237  6.3.2.52    Arabidopsis thaliana      Q8GZ29  29462792  wildtype   \n",
       "1238  6.3.2.52    Arabidopsis thaliana      Q8GZ29  29462792  wildtype   \n",
       "1239   6.3.4.4                Zea mays      O24578   9193088  wildtype   \n",
       "\n",
       "           kcat Temperature   pH                       Substrates  \\\n",
       "0      0.300000        28.0  6.5          Coniferyl acetate;NADPH   \n",
       "1      0.700000        28.0  6.5          NADPH;Coniferyl acetate   \n",
       "2      1.010000        30.0  5.0   NADPH;3-Methylcyclohexanone;H+   \n",
       "3     11.800000        30.0  5.0    3-Methylcyclohexanone;H+;NADH   \n",
       "4      0.160000        30.0  9.5       3-Methylcyclohexanol;NADP+   \n",
       "...         ...         ...  ...                              ...   \n",
       "1235   0.101667        25.0  7.0       L-Glutamate;L-Cysteine;ATP   \n",
       "1236   0.113333        25.0  7.0       ATP;L-Glutamate;L-Cysteine   \n",
       "1237   0.073333           -    -  (-)-Jasmonic acid;Glutamine;ATP   \n",
       "1238   0.066667           -    -  Glutamine;ATP;(-)-Jasmonic acid   \n",
       "1239   3.100000        22.0  7.5              IMP;L-Aspartate;GTP   \n",
       "\n",
       "                                       Products  \\\n",
       "0                      Acetate;NADP+;Isoeugenol   \n",
       "1                         Eugenol;NADP+;Acetate   \n",
       "2                    NADP+;3-Methylcyclohexanol   \n",
       "3                     NAD+;3-Methylcyclohexanol   \n",
       "4                3-Methylcyclohexanone;H+;NADPH   \n",
       "...                                         ...   \n",
       "1235  Phosphate;ADP;gamma-L-Glutamyl-L-cysteine   \n",
       "1236  gamma-L-Glutamyl-L-cysteine;ADP;Phosphate   \n",
       "1237        Diphosphate;Jasmonoyl-glutamine;AMP   \n",
       "1238        Diphosphate;AMP;Jasmonoyl-glutamine   \n",
       "1239  GDP;N6-(1,2-Dicarboxyethyl)-AMP;Phosphate   \n",
       "\n",
       "                                          substrate_IDs  \\\n",
       "0     {InChI=1S/C21H30N7O17P3/c22-17-12-19(25-7-24-1...   \n",
       "1     {InChI=1S/C21H30N7O17P3/c22-17-12-19(25-7-24-1...   \n",
       "2     {InChI=1S/p+1, InChI=1S/C7H12O/c1-6-3-2-4-7(8)...   \n",
       "3     {InChI=1S/p+1, InChI=1S/C7H12O/c1-6-3-2-4-7(8)...   \n",
       "4     {InChI=1S/C7H14O/c1-6-3-2-4-7(8)5-6/h6-8H,2-5H...   \n",
       "...                                                 ...   \n",
       "1235  {InChI=1S/C10H16N5O13P3/c11-8-5-9(13-2-12-8)15...   \n",
       "1236  {InChI=1S/C10H16N5O13P3/c11-8-5-9(13-2-12-8)15...   \n",
       "1237  {InChI=1S/C5H10N2O3/c6-3(5(9)10)1-2-4(7)8/h3H,...   \n",
       "1238  {InChI=1S/C5H10N2O3/c6-3(5(9)10)1-2-4(7)8/h3H,...   \n",
       "1239  {InChI=1S/C10H16N5O14P3/c11-10-13-7-4(8(18)14-...   \n",
       "\n",
       "                                            product_IDs  \\\n",
       "0     {InChI=1S/C10H12O2/c1-3-4-8-5-6-9(11)10(7-8)12...   \n",
       "1     {InChI=1S/C10H12O2/c1-3-4-8-5-6-9(11)10(7-8)12...   \n",
       "2     {InChI=1S/C7H14O/c1-6-3-2-4-7(8)5-6/h6-8H,2-5H...   \n",
       "3     {InChI=1S/C7H14O/c1-6-3-2-4-7(8)5-6/h6-8H,2-5H...   \n",
       "4     {InChI=1S/p+1, InChI=1S/C7H12O/c1-6-3-2-4-7(8)...   \n",
       "...                                                 ...   \n",
       "1235  {InChI=1S/H3O4P/c1-5(2,3)4/h(H3,1,2,3,4), InCh...   \n",
       "1236  {InChI=1S/H3O4P/c1-5(2,3)4/h(H3,1,2,3,4), InCh...   \n",
       "1237  {InChI=1S/H4O7P2/c1-8(2,3)7-9(4,5)6/h(H2,1,2,3...   \n",
       "1238  {InChI=1S/H4O7P2/c1-8(2,3)7-9(4,5)6/h(H2,1,2,3...   \n",
       "1239  {InChI=1S/H3O4P/c1-5(2,3)4/h(H3,1,2,3,4), InCh...   \n",
       "\n",
       "                                         Main Substrate  \\\n",
       "0     InChI=1S/C12H14O4/c1-9(13)16-7-3-4-10-5-6-11(1...   \n",
       "1     InChI=1S/C12H14O4/c1-9(13)16-7-3-4-10-5-6-11(1...   \n",
       "2     InChI=1S/C21H30N7O17P3/c22-17-12-19(25-7-24-17...   \n",
       "3     InChI=1S/C21H29N7O14P2/c22-17-12-19(25-7-24-17...   \n",
       "4     InChI=1S/C21H28N7O17P3/c22-17-12-19(25-7-24-17...   \n",
       "...                                                 ...   \n",
       "1235  InChI=1S/C5H9NO4/c6-3(5(9)10)1-2-4(7)8/h3H,1-2...   \n",
       "1236  InChI=1S/C10H16N5O13P3/c11-8-5-9(13-2-12-8)15(...   \n",
       "1237  InChI=1S/C12H18O3/c1-2-3-4-5-10-9(8-12(14)15)6...   \n",
       "1238  InChI=1S/C5H10N2O3/c6-3(5(9)10)1-2-4(7)8/h3H,1...   \n",
       "1239  InChI=1S/C10H13N4O8P/c15-6-4(1-21-23(18,19)20)...   \n",
       "\n",
       "                                               Sequence  \n",
       "0     MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...  \n",
       "1     MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...  \n",
       "2     MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...  \n",
       "3     MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...  \n",
       "4     MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...  \n",
       "...                                                 ...  \n",
       "1235  MALLSQAGGSYTVVPSGVCSKAGTKAVVSGGVRNLDVLRMKEAFGS...  \n",
       "1236  MALLSQAGGSYTVVPSGVCSKAGTKAVVSGGVRNLDVLRMKEAFGS...  \n",
       "1237  MLPKFDPTNQKACLSLLEDLTTNVKQIQDSVLEAILSRNAQTEYLR...  \n",
       "1238  MLPKFDPTNQKACLSLLEDLTTNVKQIQDSVLEAILSRNAQTEYLR...  \n",
       "1239  MSLSTLSHPAAAAAGSGKSLFPAGPAAQSVHFPKARLPVPAAVSAA...  \n",
       "\n",
       "[1240 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kcat.drop(list(set(droplist)), inplace = True)\n",
    "print(\"Dropping %s data points, because they are duplicated.\" % len(set(droplist)))\n",
    "df_kcat.reset_index(inplace = True, drop = True)\n",
    "df_kcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing top and bottom 3% of kcat values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 1649.7999999999956\n"
     ]
    }
   ],
   "source": [
    "def find_outliers_IQR(df):\n",
    "\n",
    "   q1=df.quantile(0.25)\n",
    "\n",
    "   q3=df.quantile(0.75)\n",
    "\n",
    "   IQR=q3-q1\n",
    "\n",
    "   outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
    "\n",
    "   return outliers\n",
    "\n",
    "find_outliers_IQR(df_kcat[\"kcat\"])\n",
    "\n",
    "print(df_kcat['kcat'].quantile(0.03),  df_kcat['kcat'].quantile(0.97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240\n",
      "1161\n"
     ]
    }
   ],
   "source": [
    "print(len(df_kcat))\n",
    "df_kcat = df_kcat[(df_kcat['kcat'] > df_kcat['kcat'].quantile(0.03)) & (df_kcat['kcat'] < df_kcat['kcat'].quantile(0.97))]\n",
    "df_kcat.reset_index(inplace = True, drop = True)\n",
    "print(len(df_kcat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q41736;P00221\n",
      "[281]\n",
      "Q41736;P00221\n",
      "[281, 282]\n",
      "Q41736;P00221\n",
      "[281, 282, 283]\n",
      "P19866;P12860\n",
      "[281, 282, 283, 297]\n",
      "P19866;P12860\n",
      "[281, 282, 283, 297, 298]\n",
      "O04385;O23760\n",
      "[281, 282, 283, 297, 298, 408]\n",
      "O04385;O23760\n",
      "[281, 282, 283, 297, 298, 408, 409]\n",
      "P09342;P09114\n",
      "[281, 282, 283, 297, 298, 408, 409, 436]\n",
      "P09342;P09114\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437]\n",
      "Q42588;P32260\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478]\n",
      "Q42588;P16703\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479]\n",
      "A0A2U7XUE3;Q9FEY5\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499]\n",
      "A0A2U7XUE3;Q9FEY5\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500]\n",
      "Q9SC13;P60038\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551]\n",
      "Q42588;P32260\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581]\n",
      "P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630]\n",
      "P23509;P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631]\n",
      "P23509;P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632]\n",
      "P23509;P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633]\n",
      "P23509;P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634]\n",
      "P23509;P55241\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634, 635]\n",
      "P23509;P55241\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634, 635, 636]\n",
      "P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634, 635, 636, 637]\n",
      "P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634, 635, 636, 637, 638]\n",
      "P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639]\n",
      "P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640]\n",
      "P55241;Q947C0\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641]\n",
      "P35513;P25872\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 1052]\n",
      "P35513;P25872\n",
      "[281, 282, 283, 297, 298, 408, 409, 436, 437, 478, 479, 499, 500, 551, 581, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 1052, 1053]\n"
     ]
    }
   ],
   "source": [
    "todrop= []\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    UID = df_kcat[\"Uniprot IDs\"][ind]\n",
    "    if len(UID.split(';')) > 1:\n",
    "        todrop.append(ind)\n",
    "        print(df_kcat[\"Uniprot IDs\"][ind])\n",
    "        print(todrop)\n",
    "        \n",
    "df_kcat.drop(todrop, inplace=True)\n",
    "df_kcat.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat[\"substrate_IDs\"] = df_kcat[\"substrate_IDs\"].apply(lambda x: (set(x)))\n",
    "df_kcat[\"product_IDs\"] = df_kcat[\"product_IDs\"].apply(lambda x: (set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat.to_pickle(join(datasets_dir, \"kcat_data_merged.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assigning IDs to every unique sequence and to every unique reaction in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames for all sequences and for all reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions = pd.DataFrame({\"substrates\": df_kcat[\"substrate_IDs\"],\n",
    "                            \"products\" : df_kcat[\"product_IDs\"]})\n",
    "\n",
    "df_reactions = df_reactions.loc[df_reactions[\"substrates\"] != set([])]\n",
    "df_reactions = df_reactions.loc[df_reactions[\"products\"] != set([])]\n",
    "\n",
    "\n",
    "droplist = []\n",
    "for ind in df_reactions.index:\n",
    "    sub_IDs, pro_IDs = df_reactions[\"substrates\"][ind], df_reactions[\"products\"][ind]\n",
    "    help_df = df_reactions.loc[df_reactions[\"substrates\"] == sub_IDs].loc[df_reactions[\"products\"] == pro_IDs]\n",
    "    if len(help_df):\n",
    "        for ind in list(help_df.index)[1:]:\n",
    "            droplist.append(ind)\n",
    "            \n",
    "df_reactions.drop(list(set(droplist)), inplace = True)\n",
    "df_reactions.reset_index(inplace = True, drop =True)\n",
    "\n",
    "df_reactions[\"Reaction ID\"] = [\"Reaction_\" + str(ind) for ind in df_reactions.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Sequence ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...</td>\n",
       "      <td>Sequence_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...</td>\n",
       "      <td>Sequence_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "      <td>Sequence_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...</td>\n",
       "      <td>Sequence_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAKAGENSRDKSRWSLEGMTALVTGGSKGLGEAVVEELAMLGARVH...</td>\n",
       "      <td>Sequence_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>MSSLADLINLDLSDSTDQIIAEYIWIGGSGLDMRSKARTLPGPVTD...</td>\n",
       "      <td>Sequence_455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>MSSLADLINLDLSDSTDQIIAEYIWIGGSGLDMRSKARTLPGPVTD...</td>\n",
       "      <td>Sequence_456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>MALLSQAGGSYTVVPSGVCSKAGTKAVVSGGVRNLDVLRMKEAFGS...</td>\n",
       "      <td>Sequence_457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>MLPKFDPTNQKACLSLLEDLTTNVKQIQDSVLEAILSRNAQTEYLR...</td>\n",
       "      <td>Sequence_458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>MSLSTLSHPAAAAAGSGKSLFPAGPAAQSVHFPKARLPVPAAVSAA...</td>\n",
       "      <td>Sequence_459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sequence   Sequence ID\n",
       "0    MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...    Sequence_0\n",
       "1    MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...    Sequence_1\n",
       "2    MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...    Sequence_2\n",
       "3    MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...    Sequence_3\n",
       "4    MAKAGENSRDKSRWSLEGMTALVTGGSKGLGEAVVEELAMLGARVH...    Sequence_4\n",
       "..                                                 ...           ...\n",
       "455  MSSLADLINLDLSDSTDQIIAEYIWIGGSGLDMRSKARTLPGPVTD...  Sequence_455\n",
       "456  MSSLADLINLDLSDSTDQIIAEYIWIGGSGLDMRSKARTLPGPVTD...  Sequence_456\n",
       "457  MALLSQAGGSYTVVPSGVCSKAGTKAVVSGGVRNLDVLRMKEAFGS...  Sequence_457\n",
       "458  MLPKFDPTNQKACLSLLEDLTTNVKQIQDSVLEAILSRNAQTEYLR...  Sequence_458\n",
       "459  MSLSTLSHPAAAAAGSGKSLFPAGPAAQSVHFPKARLPVPAAVSAA...  Sequence_459\n",
       "\n",
       "[460 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sequences = pd.DataFrame(data = {\"Sequence\" : df_kcat[\"Sequence\"].unique()})\n",
    "df_sequences = df_sequences.loc[~pd.isnull(df_sequences[\"Sequence\"])]\n",
    "df_sequences.reset_index(inplace = True, drop = True)\n",
    "df_sequences[\"Sequence ID\"] = [\"Sequence_\" + str(ind) for ind in df_sequences.index]\n",
    "\n",
    "df_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating maximal kcat value for each reaction and sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"max_kcat_for_RID\"] = np.nan\n",
    "for ind in df_reactions.index:\n",
    "    df_reactions[\"max_kcat_for_RID\"][ind] = max(df_kcat.loc[df_kcat[\"substrate_IDs\"] == df_reactions[\"substrates\"][ind]].loc[df_kcat[\"product_IDs\"] == df_reactions[\"products\"][ind]][\"kcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences[\"max_kcat_for_UID\"] = np.nan\n",
    "for ind in df_sequences.index:\n",
    "    df_sequences[\"max_kcat_for_UID\"][ind] = max(df_kcat.loc[df_kcat[\"Sequence\"] == df_sequences['Sequence'][ind]][\"kcat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the sum of the molecular weights of all substrates and of all products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mw_mets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_149149/3629873119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mproducts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_reactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"products\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmw_subs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmw_mets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetabolites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubstrates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmw_pros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmw_mets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetabolites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproducts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mw_mets' is not defined"
     ]
    }
   ],
   "source": [
    "df_reactions[\"MW_frac\"] = np.nan\n",
    "\n",
    "for ind in df_reactions.index:\n",
    "    substrates = list(df_reactions[\"substrates\"][ind])\n",
    "    products = list(df_reactions[\"products\"][ind])\n",
    "    \n",
    "    mw_subs = mw_mets(metabolites = substrates)\n",
    "    mw_pros = mw_mets(metabolites = products)\n",
    "    \n",
    "    if mw_subs == np.nan or mw_pros == np.nan:\n",
    "        df_reactions[\"MW_frac\"][ind] = np.inf\n",
    "    if mw_pros != 0:\n",
    "        df_reactions[\"MW_frac\"][ind] = mw_subs/mw_pros\n",
    "    else:\n",
    "        df_reactions[\"MW_frac\"][ind] = np.inf\n",
    "        \n",
    "df_reactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating enzyme, reaction and substrate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/esm/zipball/main\" to /home/jearle/.cache/torch/hub/main.zip\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'esmfold_structure_module_only_8M' from 'esm.pretrained' (/zfs/omics/personal/jearle/miniconda3/envs/ee/lib/python3.7/site-packages/esm/pretrained.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_149149/4258868159.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facebookresearch/esm:main\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"esm2_t33_650M_UR50D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/zfs/omics/personal/jearle/miniconda3/envs/ee/lib/python3.7/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m                                            verbose=verbose, skip_validation=skip_validation)\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zfs/omics/personal/jearle/miniconda3/envs/ee/lib/python3.7/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0mhubconf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0mhub_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhubconf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zfs/omics/personal/jearle/miniconda3/envs/ee/lib/python3.7/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m_import_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zfs/omics/personal/jearle/miniconda3/envs/ee/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/zfs/omics/personal/jearle/miniconda3/envs/ee/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/facebookresearch_esm_main/hubconf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from esm.pretrained import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mesm1_t6_43M_UR50S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mesm1_t12_85M_UR50S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'esmfold_structure_module_only_8M' from 'esm.pretrained' (/zfs/omics/personal/jearle/miniconda3/envs/ee/lib/python3.7/site-packages/esm/pretrained.py)"
     ]
    }
   ],
   "source": [
    "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating model input:\n",
    "df_sequences[\"model_input\"] = [seq[:1022] for seq in df_sequences[\"Sequence\"]]\n",
    "model_input = [(df_sequences[\"Sequence ID\"][ind], df_sequences[\"model_input\"][ind]) for ind in df_sequences.index]\n",
    "seqs = [model_input[i][1] for i in range(len(model_input))]\n",
    "#loading ESM-2 model:\n",
    "print(\".....2(a) Loading ESM-2 model.\")\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "#convert input into batches:\n",
    "\n",
    "#Calculate ESM-2 representations\n",
    "print(\".....2(b) Calculating enzyme representations.\")\n",
    "df_sequences[\"Enzyme rep\"] = \"\"\n",
    "\n",
    "for ind in df_sequences.index:\n",
    "    print(ind,\"/\",len(df_sequences))    \n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter([(df_sequences[\"Sequence ID\"][ind], df_sequences[\"model_input\"][ind])])\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33])\n",
    "    df_sequences[\"Enzyme rep\"][ind] = results[\"representations\"][33][0, 1 : len(df_sequences[\"model_input\"][ind]) + 1].mean(0).numpy()\n",
    "    \n",
    "df_sequences.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metabolite_type(met):\n",
    "    if is_KEGG_ID(met):\n",
    "        return(\"KEGG\")\n",
    "    elif is_InChI(met):\n",
    "        return(\"InChI\")\n",
    "    else:\n",
    "        return(\"invalid\")\n",
    "\n",
    "def get_reaction_site_smarts(metabolites):\n",
    "    reaction_site = \"\"\n",
    "    for met in metabolites:\n",
    "        met_type = get_metabolite_type(met)\n",
    "        if met_type == \"KEGG\":\n",
    "            try:\n",
    "                Smarts = Chem.MolToSmarts(Chem.MolFromMolFile(join(\"\", \"\", \"data\", \"mol-files\",  met + \".mol\")))\n",
    "            except OSError:\n",
    "                return(np.nan)\n",
    "        elif met_type == \"InChI\":\n",
    "            Smarts = Chem.MolToSmarts(Chem.inchi.MolFromInchi(met))\n",
    "        else:\n",
    "            Smarts = \"invalid\"\n",
    "        reaction_site = reaction_site + \".\" + Smarts\n",
    "    return(reaction_site[1:])\n",
    "\n",
    "\n",
    "def is_KEGG_ID(met):\n",
    "    #a valid KEGG ID starts with a \"C\" or \"D\" followed by a 5 digit number:\n",
    "    if len(met) == 6 and met[0] in [\"C\", \"D\"]:\n",
    "        try:\n",
    "            int(met[1:])\n",
    "            return(True)\n",
    "        except: \n",
    "            pass\n",
    "    return(False)\n",
    "\n",
    "def is_InChI(met):\n",
    "    m = Chem.inchi.MolFromInchi(met,sanitize=False)\n",
    "    if m is None:\n",
    "      return(False)\n",
    "    else:\n",
    "      try:\n",
    "        Chem.SanitizeMol(m)\n",
    "      except:\n",
    "        print('.......Metabolite string \"%s\" is in InChI format but has invalid chemistry' % met)\n",
    "        return(False)\n",
    "    return(True)\n",
    "\n",
    "def convert_fp_to_array(difference_fp_dict):\n",
    "    fp = np.zeros(2048)\n",
    "    for key in difference_fp_dict.keys():\n",
    "        fp[key] = difference_fp_dict[key]\n",
    "    return(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"difference_fp\"], df_reactions[\"structural_fp\"],  = \"\", \"\"\n",
    "for ind in df_reactions.index:\n",
    "    left_site = get_reaction_site_smarts(df_reactions[\"substrates\"][ind])\n",
    "    right_site = get_reaction_site_smarts(df_reactions[\"products\"][ind])\n",
    "    if not pd.isnull(left_site) and not pd.isnull(right_site):\n",
    "        rxn_forward = AllChem.ReactionFromSmarts(left_site + \">>\" + right_site)\n",
    "        difference_fp = Chem.rdChemReactions.CreateDifferenceFingerprintForReaction(rxn_forward)\n",
    "        difference_fp = convert_fp_to_array(difference_fp.GetNonzeroElements())\n",
    "        df_reactions[\"difference_fp\"][ind] = difference_fp\n",
    "        df_reactions[\"structural_fp\"][ind] = Chem.rdChemReactions.CreateStructuralFingerprintForReaction(rxn_forward).ToBitString()\n",
    "\n",
    "df_reactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences.to_pickle(join(datasets_dir, \"all_sequences_with_IDs.pkl\"))\n",
    "df_reactions.to_pickle(join(datasets_dir, \"all_reactions_with_IDs.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences[\"max_kcat_for_UID\"] = np.nan\n",
    "for ind in df_sequences.index:\n",
    "    df_sequences[\"max_kcat_for_UID\"][ind] = max(df_kcat.loc[df_kcat[\"Sequence\"] == df_sequences['Sequence'][ind]][\"kcat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Sequence and Reaction IDs to kcat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat = df_kcat.merge(df_sequences, on = \"Sequence\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions.rename(columns = {\"substrates\" : \"substrate_IDs\",\n",
    "                              \"products\" : \"product_IDs\"}, inplace = True)\n",
    "\n",
    "df_kcat[\"Reaction ID\"] = np.nan\n",
    "df_kcat[\"MW_frac\"] = np.nan\n",
    "df_kcat[\"max_kcat_for_RID\"] = np.nan\n",
    "df_kcat[\"difference_fp\"] = \"\"\n",
    "df_kcat[\"structural_fp\"] = \"\"\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    sub_set, pro_set = df_kcat[\"substrate_IDs\"][ind], df_kcat[\"product_IDs\"][ind]\n",
    "    \n",
    "    help_df = df_reactions.loc[df_reactions[\"substrate_IDs\"] == sub_set].loc[df_reactions[\"product_IDs\"] == pro_set]\n",
    "    if len(help_df) == 1:\n",
    "        df_kcat[\"Reaction ID\"][ind] = list(help_df[\"Reaction ID\"])[0]\n",
    "        df_kcat[\"max_kcat_for_RID\"][ind] = list(help_df[\"max_kcat_for_RID\"])[0]\n",
    "        df_kcat[\"MW_frac\"][ind] = list(help_df[\"MW_frac\"])[0]\n",
    "        df_kcat[\"difference_fp\"][ind] = list(help_df[\"difference_fp\"])[0]\n",
    "        df_kcat[\"structural_fp\"][ind] = list(help_df[\"structural_fp\"])[0]\n",
    "df_kcat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat[\"MACCS FP\"] = \"\"\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    id = df_kcat[\"Main Substrate\"][ind]\n",
    "    if id[0] == \"C\":\n",
    "        try:\n",
    "            mol = Chem.MolFromMolFile(join(datasets_dir,\"mol-files\", id + '.mol'))\n",
    "        except OSError:\n",
    "            None\n",
    "    else:\n",
    "        try:\n",
    "            mol = Chem.inchi.MolFromInchi(id,sanitize=False)\n",
    "        except OSError:\n",
    "            None\n",
    "    if mol is not None:\n",
    "        maccs_fp = MACCSkeys.GenMACCSKeys(mol).ToBitString()\n",
    "        df_kcat[\"MACCS FP\"][ind] = maccs_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the maximal kcat value for every EC number in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EC_kcat = pd.read_csv(join(datasets_dir, \"max_EC_\" + organism + \".tsv\"), sep = \"\\t\", header=0)\n",
    "\n",
    "df_EC_kcat.head(5)\n",
    "df_kcat[\"max_kcat_for_EC\"] = np.nan\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    EC = df_kcat[\"ECs\"][ind]\n",
    "    max_kcat = 0\n",
    "    try:\n",
    "        print(EC)\n",
    "        max_kcat = df_EC_kcat.loc[df_EC_kcat[\"EC\"] == EC, \"max_kcat\"].iloc[0]\n",
    "        print(max_kcat)\n",
    "    except:\n",
    "        pass\n",
    "    if max_kcat != 0:\n",
    "        df_kcat[\"max_kcat_for_EC\"][ind] = max_kcat\n",
    "df_kcat.to_pickle(join(datasets_dir, \"merged_and_grouped_kcat_dataset2.pkl\"))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing non-optimally measured values\n",
    "\n",
    "To ignore $kcat$ values that were obtained under non-optimal conditions, we exclude values lower than 0.1\\% than the maximal $kcat$ value for the same enzyme, reaction or EC number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat[\"frac_of_max_UID\"] = np.nan\n",
    "df_kcat[\"frac_of_max_RID\"] = np.nan\n",
    "df_kcat[\"frac_of_max_EC\"] = np.nan\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    df_kcat[\"frac_of_max_UID\"][ind] =  df_kcat[\"kcat\"][ind]/df_kcat[\"max_kcat_for_UID\"][ind]\n",
    "    df_kcat[\"frac_of_max_RID\"][ind] =  df_kcat[\"kcat\"][ind]/df_kcat[\"max_kcat_for_RID\"][ind]\n",
    "    df_kcat[\"frac_of_max_EC\"][ind] = df_kcat[\"kcat\"][ind]/df_kcat[\"max_kcat_for_EC\"][ind]\n",
    "\n",
    "len(df_kcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_kcat)\n",
    "\n",
    "df_kcat = df_kcat.loc[df_kcat[\"frac_of_max_UID\"] >= 0.01]\n",
    "df_kcat = df_kcat.loc[df_kcat[\"frac_of_max_RID\"] >= 0.01]\n",
    "\n",
    "df_kcat = df_kcat.loc[df_kcat[\"frac_of_max_EC\"] <= 10]\n",
    "df_kcat = df_kcat.loc[df_kcat[\"frac_of_max_EC\"] >= 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We remove %s data points, because we suspect that these kcat values were not measure for the natural reaction \" \\\n",
    "    \"of an enzyme or under non-optimal conditions.\" % (n-len(df_kcat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing data points with reaction queations with uneven fraction of molecular weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_kcat)\n",
    "\n",
    "df_kcat = df_kcat.loc[df_kcat[\"MW_frac\"] < 3]\n",
    "df_kcat = df_kcat.loc[df_kcat[\"MW_frac\"] > 1/3]\n",
    "\n",
    "print(\"We remove %s data points because the sum of molecular weights of substrates does not match the sum of molecular\" \\\n",
    "      \"weights of the products.\" % (n-len(df_kcat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of final kcat dataset: %s\" % len(df_kcat))\n",
    "df_kcat.to_pickle(join(datasets_dir, \"final_kcat_dataset_\" + organism + \".pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing dataset and splitting into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat = pd.read_pickle(join(datasets_dir, \"final_kcat_dataset_\" + organism + \".pkl\"))\n",
    "df_kcat[\"log10_kcat\"] = [np.log10(x) for x in df_kcat[\"kcat\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making input for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inchi_ids = {}\n",
    "for i, element in enumerate(df_kcat[\"Main Substrate\"]):\n",
    "    if element[0] != 'C' and element not in inchi_ids.keys():\n",
    "        inchi_ids[element] = str(i)\n",
    "        mol = Chem.inchi.MolFromInchi(element)\n",
    "        if not mol is None:\n",
    "            calculate_atom_and_bond_feature_vectors(mol, str(i))\n",
    "        Chem.rdmolfiles.MolToMolFile(Chem.inchi.MolFromInchi(element), join(datasets_dir,\"mol-files\", str(i) + \".mol\")  )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting glucosinolates into validation dataset\n",
    "\n",
    "Search UniProt for GO term related to glucosionalte metabolic process, download file as .tsv and filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucosinolates = pd.read_table(join(datasets_dir,\"glucosinolates.tsv\"))[\"Entry\"].tolist()\n",
    "df_validation = df_kcat[df_kcat[\"Uniprot IDs\"].isin(glucosinolates)]\n",
    "df_validation.reset_index(inplace=True, drop = True)\n",
    "df_kcat = df_kcat[~df_kcat[\"Uniprot IDs\"].isin(glucosinolates)]\n",
    "df_kcat.reset_index(inplace=True, drop = True)\n",
    "split = \"full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing with only Arabidopsis data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kcat = df_kcat[df_kcat[\"Organism\"] == 'Arabidopsis thaliana']\n",
    "# df_kcat.reset_index(inplace=True, drop = True)\n",
    "# split = \"Arabidopsis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing with only Brassicaceae data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncbi = NCBITaxa()\n",
    "\n",
    "# organisms = {}\n",
    "\n",
    "# def is_brassicaceae(org):\n",
    "#     try:\n",
    "#         tax_id = ncbi.get_name_translator([org])[org][0]\n",
    "#         lineage = ncbi.get_lineage(tax_id)\n",
    "#         if 3700 not in lineage:\n",
    "#             return(False)\n",
    "#         else:\n",
    "#             return(True)\n",
    "#     except KeyError:\n",
    "#         return(False)\n",
    "    \n",
    "# for org in df_kcat[\"Organism\"].tolist():\n",
    "#     if org not in organisms.keys():\n",
    "#         organisms[org] = is_brassicaceae(org)\n",
    "\n",
    "# df_kcat = df_kcat[df_kcat[\"Organism\"].isin([key for key, value in organisms.items() if value is True])]\n",
    "# df_kcat.reset_index(inplace=True, drop = True)\n",
    "# split = \"Brassicaceae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing only with wildtype data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kcat = df_kcat[df_kcat[\"Type\"].str.contains(\"wildtype\")]\n",
    "# df_kcat.reset_index(inplace=True, drop = True)\n",
    "# split = \"wildtype\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing only with secondary metabolite data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secondary = pd.read_table(join(datasets_dir,\"secondary_metabolites.tsv\"))[\"Entry\"].tolist()\n",
    "# df_kcat = df_kcat[df_kcat[\"Uniprot IDs\"].isin(secondary)]\n",
    "# df_kcat.reset_index(inplace=True, drop = True)\n",
    "# split = \"secondary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(join(datasets_dir, \"splits\", split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_kcat.copy()\n",
    "df = df.sample(frac = 1, random_state=123)\n",
    "df.reset_index(drop= True, inplace = True)\n",
    "\n",
    "train_df, test_df = split_dataframe_enzyme(frac = 5, df = df.copy())\n",
    "print(\"Test set size: %s\" % len(test_df))\n",
    "print(\"Training set size: %s\" % len(train_df))\n",
    "print(\"Size of test set in percent: %s\" % np.round(100*len(test_df)/ (len(test_df) + len(train_df))))\n",
    "\n",
    "train_df.reset_index(inplace = True, drop = True)\n",
    "test_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "train_df.to_pickle(join(datasets_dir, \"splits\", split, \"train_df_kcat_%s.pkl\" %organism))\n",
    "test_df.to_pickle(join(datasets_dir, \"splits\", split, \"test_df_kcat_%s.pkl\" %organism))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train2 = train_df.copy()\n",
    "data_train2[\"index\"] = list(data_train2.index)\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=5)\n",
    "indices_fold1 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold1))#\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=4)\n",
    "indices_fold2 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold2))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=3)\n",
    "indices_fold3 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold3))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=2)\n",
    "indices_fold4 = list(df_fold[\"index\"])\n",
    "indices_fold5 = list(data_train2[\"index\"])\n",
    "print(len(data_train2), len(indices_fold4))\n",
    "\n",
    "\n",
    "fold_indices = [indices_fold1, indices_fold2, indices_fold3, indices_fold4, indices_fold5]\n",
    "\n",
    "CV_train_indices = [[], [], [], [], []]\n",
    "CV_test_indices = [[], [], [], [], []]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            CV_train_indices[i] = CV_train_indices[i] + fold_indices[j]\n",
    "    CV_test_indices[i] = fold_indices[i]\n",
    "    \n",
    "    \n",
    "np.save(join(datasets_dir, \"splits\", split, \"CV_train_indices_%s\" %organism), CV_train_indices)\n",
    "np.save(join(datasets_dir, \"splits\", split, \"CV_test_indices_%s\" %organism), CV_test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building GNN for substrate representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "\n",
    "for ind in train_df.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"train_\" + str(ind), df = train_df,\n",
    "                                      save_folder = join(datasets_dir, \"GNN_input_data\", split))\n",
    "    \n",
    "for ind in test_df.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"test_\" + str(ind), df = test_df,\n",
    "                                      save_folder = join(datasets_dir, \"GNN_input_data\", split))\n",
    "    \n",
    "for ind in df_validation.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"val_\" + str(ind), df = df_validation,\n",
    "                                    save_folder = join(datasets_dir, \"GNN_input_data\", split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "train_indices = [index[:index.rfind(\"_\")] for index in train_indices]\n",
    "train_indices = list(set([index for index in train_indices if \"train\" in index]))\n",
    "\n",
    "test_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "test_indices = [index[:index.rfind(\"_\")] for index in test_indices]\n",
    "test_indices = list(set([index for index in test_indices if \"test\" in index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter optimization with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'batch_size': [32,64,96],\n",
    "                'D': [50,100],\n",
    "                'learning_rate': [0.01, 0.1],\n",
    "                'epochs': [30,50,80],\n",
    "                'l2_reg_fc' : [0.01, 0.1, 1],\n",
    "                'l2_reg_conv': [0.01, 0.1, 1],\n",
    "                'rho': [0.9, 0.95, 0.99]}\n",
    "\n",
    "params_list = [(batch_size, D, learning_rate, epochs, l2_reg_fc, l2_reg_conv, rho) for batch_size in param_grid['batch_size'] for D in param_grid[\"D\"] for learning_rate in param_grid['learning_rate']\n",
    "                for epochs in param_grid['epochs'] for l2_reg_fc in param_grid['l2_reg_fc'] for l2_reg_conv in param_grid['l2_reg_conv'] for rho in param_grid[\"rho\"]]\n",
    "\n",
    "params_list = random.sample(params_list, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "results=[]\n",
    "\n",
    "for params in params_list:\n",
    "\n",
    "    batch_size, D, learning_rate, epochs, l2_reg_fc, l2_reg_conv, rho = params\n",
    "    count +=1\n",
    "    MAE = []\n",
    "\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = CV_train_indices[i], CV_test_indices[i]\n",
    "        train_index = [ind for ind in train_indices if int(ind.split(\"_\")[1]) in train_index]\n",
    "        test_index = [ind for ind in train_indices if int(ind.split(\"_\")[1]) in test_index]\n",
    "\n",
    "        train_params = {'batch_size': batch_size,\n",
    "                    'folder' :join(datasets_dir, \"GNN_input_data/full\"),\n",
    "                    'list_IDs' : np.array(train_index),\n",
    "                    'shuffle': True}\n",
    "\n",
    "        test_params = {'batch_size': len(test_index),\n",
    "                    'folder' : join(datasets_dir, \"GNN_input_data/full\"),\n",
    "                    'list_IDs' : np.array(test_index),\n",
    "                    'shuffle': False}\n",
    "\n",
    "        training_generator = DataGenerator(**train_params)\n",
    "        test_generator = DataGenerator(**test_params)\n",
    "\n",
    "\n",
    "        model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "                        D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "        model.fit(training_generator, epochs= epochs, shuffle = True, verbose = 1)\n",
    "\n",
    "        #get test_y:\n",
    "        test_indices_y = [int(ind.split(\"_\")[1]) for ind in train_indices if ind in test_index]\n",
    "        test_y = np.array([train_df[\"kcat\"][ind] for ind in test_indices_y])\n",
    "\n",
    "        pred_test = model.predict(test_generator)\n",
    "        mae = np.median(abs(np.array([10**x for x in pred_test]) - np.reshape(test_y[:len(pred_test)], (-1,1))))\n",
    "        print(mae)\n",
    "        MAE.append(mae)\n",
    "\n",
    "    results.append({\"batch_size\" : batch_size, \"D\" : D , \"learning_rate\" : learning_rate, \"epochs\" : epochs,\n",
    "                    \"l2_reg_fc\" : l2_reg_fc, \"l2_reg_conv\" : l2_reg_conv, \"rho\" : rho, \"cv_mae\" : np.mean(MAE)})\n",
    "\n",
    "params = min(results, key=lambda d: d['cv_mae'])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'batch_size': 32, 'D': 50, 'learning_rate': 0.01, 'epochs': 30, 'l2_reg_fc': 0.1, 'l2_reg_conv': 1, 'rho': 0.9, 'cv_mae': 2.4853503725624084}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model with the best set of hyperparmeters on the whole training set and validate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "D = 50\n",
    "learning_rate = 0.01\n",
    "epochs = 30\n",
    "l2_reg_fc = 0.1\n",
    "l2_reg_conv = 1\n",
    "rho = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = os.listdir(join(datasets_dir, \"GNN_input_data/full\"))\n",
    "train_indices = [index[:index.rfind(\"_\")] for index in train_indices]\n",
    "train_indices = list(set([index for index in train_indices if \"train\" in index]))\n",
    "\n",
    "test_indices = os.listdir(join(datasets_dir, \"GNN_input_data/full\"))\n",
    "test_indices = [index[:index.rfind(\"_\")] for index in test_indices]\n",
    "test_indices = list(set([index for index in test_indices if \"test\" in index]))\n",
    "\n",
    "train_params = {'batch_size': batch_size,\n",
    "              'folder' :join(datasets_dir, \"GNN_input_data/full\"),\n",
    "              'list_IDs' : train_indices,\n",
    "              'shuffle': True}\n",
    "\n",
    "test_params = {'batch_size': batch_size,\n",
    "              'folder' :join(datasets_dir, \"GNN_input_data/full\"),\n",
    "              'list_IDs' : test_indices,\n",
    "              'shuffle': False}\n",
    "\n",
    "training_generator = DataGenerator(**train_params)\n",
    "test_generator = DataGenerator(**test_params)\n",
    "\n",
    "model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "                  D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "\n",
    "model.fit(training_generator, epochs= epochs, shuffle = True, verbose = 1)\n",
    "model.save_weights(join(datasets_dir, \"model_weights\", \"saved_model_GNN_best_hyperparameters\"))\n",
    "\n",
    "pred_test = model.predict(test_generator)\n",
    "test_indices_y = [int(ind.split(\"_\")[1]) for ind in np.array(test_indices)]\n",
    "test_y = np.array([test_df[\"kcat\"][ind] for ind in test_indices_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating substrate representation for every data point in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "                  D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "model.load_weights(join(datasets_dir, \"model_weights\", \"saved_model_GNN_best_hyperparameters\"))\n",
    "\n",
    "get_fingerprint_fct = K.function([model.layers[0].input, model.layers[26].input,\n",
    "                                  model.layers[3].input],\n",
    "                                  [model.layers[-10].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_folder = join(datasets_dir, \"GNN_input_data\", split)   \n",
    "\n",
    "def get_representation_input(cid_list):\n",
    "    XE = ();\n",
    "    X = ();\n",
    "    A = ();\n",
    "    # Generate data\n",
    "    for cid in cid_list:\n",
    "        try:\n",
    "            X = X + (np.load(join(input_data_folder, cid + '_X.npy')), );\n",
    "            XE = XE + (np.load(join(input_data_folder, cid + '_XE.npy')), );\n",
    "            A = A + (np.load(join(input_data_folder, cid + '_A.npy')), );\n",
    "        except FileNotFoundError: #return zero arrays:\n",
    "            X = X + (np.zeros((N,32)), );\n",
    "            XE = XE + (np.zeros((N,N,F)), );\n",
    "            A = A + (np.zeros((N,N,1)), );\n",
    "    return(XE, X, A)\n",
    "\n",
    "input_data_folder = join(datasets_dir, \"GNN_input_data\", split)   \n",
    "def get_substrate_representations(df, training_set, testing_set, get_fingerprint_fct):\n",
    "    df[\"GNN FP\"] = \"\"\n",
    "    i = 0\n",
    "    n = len(df)\n",
    "    \n",
    "    cid_all = list(df.index)\n",
    "    if training_set == True:\n",
    "        prefix = \"train_\"\n",
    "    elif testing_set == True:\n",
    "        prefix = \"test_\"\n",
    "    else:\n",
    "        prefix = \"val_\"\n",
    "    cid_all = [prefix + str(cid) for cid in cid_all]\n",
    "    \n",
    "    while i*32 <= n:\n",
    "        if (i+1)*32  <= n:\n",
    "            XE, X, A = get_representation_input(cid_all[i*32:(i+1)*32])\n",
    "            representations = get_fingerprint_fct([np.array(XE), np.array(X),np.array(A)])[0]\n",
    "            df[\"GNN FP\"][i*32:(i+1)*32] = list(representations[:, :52])\n",
    "        else:\n",
    "            print(i)\n",
    "            XE, X, A = get_representation_input(cid_all[-min(32,n):])\n",
    "            representations = get_fingerprint_fct([np.array(XE), np.array(X),np.array(A)])[0]\n",
    "            df[\"GNN FP\"][-min(32,n):] = list(representations[:, :52])\n",
    "        i += 1\n",
    "        \n",
    "    ### set all GNN FP-entries with no input matrices to np.nan:\n",
    "    all_X_matrices = os.listdir(input_data_folder)\n",
    "    for ind in df.index:\n",
    "        if prefix +str(ind) +\"_X.npy\" not in all_X_matrices:\n",
    "            df[\"GNN FP\"][ind] = np.nan\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the GNN representations\n",
    "train_with_rep = get_substrate_representations(df = train_df, training_set = True, testing_set = False,\n",
    "                                                      get_fingerprint_fct = get_fingerprint_fct)\n",
    "test_with_rep = get_substrate_representations(df = test_df, training_set = False, testing_set = True,\n",
    "                                                     get_fingerprint_fct = get_fingerprint_fct)\n",
    "val_with_rep = get_substrate_representations(df = df_validation, training_set = False, testing_set = False,\n",
    "                                                     get_fingerprint_fct = get_fingerprint_fct)\n",
    "\n",
    "#Saving the DataFrames:\n",
    "train_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "test_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "val_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
